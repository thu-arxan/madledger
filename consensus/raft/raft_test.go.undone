package raft

import (
	"context"
	"crypto/tls"
	"crypto/x509"
	"errors"
	"fmt"
	"io/ioutil"
	"madledger/common/util"
	core "madledger/core/types"
	raftpb "madledger/protos"
	"os"
	"reflect"
	"sort"
	"sync"
	"testing"
	"time"

	"github.com/stretchr/testify/require"
	"google.golang.org/grpc"
	"google.golang.org/grpc/credentials"
)

var (
	blocks    []*core.Block
	beginPort = 12345
)

func init() {
	createBlocks(10000)
}

func TestSolo(t *testing.T) {
	initSoloPath()
	cfg, err := newRaftConfg(1, beginPort, getSoloPath(), map[uint64]string{
		1: fmt.Sprintf("localhost:%d", beginPort),
	}, false)
	require.NoError(t, err)
	node, err := NewRaft(cfg)
	require.NoError(t, err)
	require.NoError(t, startCluster([]*Raft{node}))
	node.Stop()
	// Start again and add some blocks
	require.NoError(t, startCluster([]*Raft{node}))
	require.NoError(t, addBlocks([]*Raft{node}, 0, 100, false))
	node.Stop()
	// Start again and add some blocks to make sure the blockchain system will not get old blocks
	require.NoError(t, startCluster([]*Raft{node}))
	require.NoError(t, addBlocks([]*Raft{node}, 100, 110, false))
	node.Stop()
	// Using another way to restart
	node, err = NewRaft(cfg)
	require.NoError(t, err)
	require.NoError(t, startCluster([]*Raft{node}))
	require.NoError(t, addBlocks([]*Raft{node}, 110, 120, false))
	node.Stop()
}

// Solo TLS is not as complicated as without tls, it just add some blocks to make sure it works when tls enabled
func TestSoloTLS(t *testing.T) {
	initSoloPath()
	cfg, err := newRaftConfg(1, beginPort, getSoloPath(), map[uint64]string{
		1: fmt.Sprintf("localhost:%d", beginPort),
	}, true)
	require.NoError(t, err)
	node, err := NewRaft(cfg)
	require.NoError(t, err)
	require.NoError(t, startCluster([]*Raft{node}))
	require.NoError(t, addBlocks([]*Raft{node}, 0, 100, false))
	node.Stop()
	initSoloPath()
}

func TestCluster(t *testing.T) {
	initClusterPath()
	cluster, err := newCluster(false)
	require.NoError(t, err)
	require.NoError(t, startCluster(cluster))
	require.NoError(t, addBlocks(cluster, 0, 100, false))
	require.NoError(t, stopCluster(cluster))
	cluster, err = newCluster(false)
	require.NoError(t, err)
	require.NoError(t, startCluster(cluster))
	var wg sync.WaitGroup
	wg.Add(1)
	go func() {
		defer wg.Done()
		require.NoError(t, restartNode(cluster))
	}()
	wg.Add(1)
	go func() {
		defer wg.Done()
		require.NoError(t, addBlocks(cluster, 100, 200, true))
	}()
	wg.Wait()
	require.NoError(t, stopCluster(cluster))
	initClusterPath()
}

// Cluster TLS is not as complicated as without tls, it just add some blocks to make sure it works when tls enabled
func TestClusterTLS(t *testing.T) {
	initClusterPath()
	cluster, err := newCluster(true)
	require.NoError(t, err)
	require.NoError(t, startCluster(cluster))
	require.NoError(t, addBlocks(cluster, 0, 100, false))
	require.NoError(t, stopCluster(cluster))
	initClusterPath()
}

func TestConfChange(t *testing.T) {
	// It will random choose tls or non-tls
	var tlsEnable = false
	if util.RandNum(2) != 0 {
		tlsEnable = true
	}
	initClusterPath()
	cluster, err := newCluster(tlsEnable)
	require.NoError(t, err)
	require.NoError(t, startCluster(cluster))
	var blockSize = 100
	// add some blocks
	require.NoError(t, addBlocks(cluster, 0, blockSize, false))
	// then add a new node
	cluster, err = addNode(cluster, tlsEnable)
	require.NoError(t, err)
	// then wait a while and check if the new node restore all the blocks
	time.Sleep(500 * time.Millisecond)
	require.Equal(t, blockSize, len(cluster[3].app.blocks))
	require.Equal(t, cluster[0].app.blocks, cluster[3].app.blocks)
	// then random remove a peer
	var i = util.RandNum(len(cluster))
	var rn = cluster[i]
	cluster, err = removeNode(cluster, rn.cfg.id)
	require.NoError(t, err)
	// do not forget stop the node manual
	rn.Stop()
	// then add some blocks
	require.NoError(t, addBlocks(cluster, blockSize, 2*blockSize, false))
	time.Sleep(500 * time.Millisecond)
	for i := range cluster {
		require.Len(t, cluster[i].app.blocks, 2*blockSize)
		if i != 0 {
			require.Equal(t, cluster[0].app.blocks, cluster[i].app.blocks)
		}
	}
	require.NoError(t, stopCluster(cluster))
	initClusterPath()
}

// createBlocks will set blocks for using
func createBlocks(size int) {
	blocks = make([]*core.Block, size)
	var prevHash = []byte{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}
	for i := range blocks {
		// todo: do real test
		blocks[i] = core.NewBlock("", uint64(i), prevHash, nil)
		prevHash = blocks[i].Hash()
	}
}

// addBlocks will add blocks belong to [begin, end)
func addBlocks(cluster []*Raft, begin int, end int, sleep bool) error {
	if end <= begin {
		return errors.New("End is no bigger than begin")
	}
	var errCh = make(chan error, 1)
	var wg sync.WaitGroup
	// then try to add these blocks
	wg.Add(1)
	go func() {
		defer wg.Done()
		var leader int
		for i := begin; i < end; i++ {
			if sleep {
				time.Sleep(100 * time.Millisecond)
			}
			if err := addBlock(cluster, blocks[i], &leader); err != nil {
				errCh <- err
				return
			}
		}
		// fmt.Printf("Add blocks [%d, %d) done\n", begin, end)
	}()
	// start some threads to receive all blocks
	for i := range cluster {
		node := cluster[i]
		wg.Add(1)
		go func() {
			defer wg.Done()
			var prevBlock *core.Block
			for {
				select {
				case block := <-node.BlockCh():
					// fmt.Printf("[%d] Receive block %d\n", node.cfg.id, block.GetNumber())
					if prevBlock == nil {
						if block.GetNumber() < uint64(begin) {
							// ignore the block
							// errCh <- fmt.Errorf("[%d] Block should begin from %d other than %d", node.cfg.id, begin, block.GetNumber())
							// return
							continue
						}
						if block.GetNumber() > uint64(begin) {
							node.NotifyLater(block)
							continue
						}
					} else {
						if block.GetNumber() < prevBlock.GetNumber()+1 {
							continue
						} else if block.GetNumber() > prevBlock.GetNumber()+1 {
							node.NotifyLater(block)
							continue
						}
					}
					// fmt.Printf("[%d] Succeed receive block %d\n", node.cfg.id, block.GetNumber())
					// node.FetchBlockDone(block.GetNumber())
					if block.GetNumber() == uint64(end-1) {
						return
					}
					prevBlock = block
				}
			}

		}()
	}

	go func() {
		wg.Wait()
		// then check if all app blocks are same
		for i := 1; i < len(cluster); i++ {
			if !reflect.DeepEqual(cluster[i-1].app.blocks, cluster[i].app.blocks) {
				printBlocks(cluster[i-1].app.blocks)
				printBlocks(cluster[i].app.blocks)
				errCh <- fmt.Errorf("Blocks of app[%d] and app[%d] is different", i-1, i)
				return
			}
		}
		errCh <- nil
	}()
	select {
	case err := <-errCh:
		return err
	}
}

func printBlocks(blocks map[uint64]*core.Block) {
	var nums = make([]uint64, 0)
	for num := range blocks {
		nums = append(nums, num)
	}
	sort.Slice(nums, func(i, j int) bool {
		return nums[i] < nums[j]
	})
	fmt.Println(nums)
}

// addBlock trys to add a block into the cluster
func addBlock(cluster []*Raft, block *core.Block, leader *int) error {
	if leader == nil {
		return errors.New("The pointer to the leader can not be nil")
	}
	for {
		node := cluster[(*leader)]
		err := node.AddBlock(block)
		if err == nil {
			return nil
		}
		(*leader) = util.RandNum(len(cluster))
	}
}

// newCluster create a cluster contain three machines
func newCluster(tlsEnable bool) ([]*Raft, error) {
	var cluster = make([]*Raft, 3)
	// set peers
	peers := make(map[uint64]string)
	for i := 1; i <= len(cluster); i++ {
		peers[uint64(i)] = fmt.Sprintf("localhost:%d", beginPort+2+3*(i-1))
	}
	for i := range cluster { // i begin from 0
		cfg, err := newRaftConfg(uint64(i+1), beginPort+3*i, fmt.Sprintf("%s/%d/raft", getClusterPath(), i+1), peers, tlsEnable)
		if err != nil {
			return cluster, err
		}
		cluster[i], err = NewRaft(cfg)
		if err != nil {
			return cluster, err
		}
	}
	return cluster, nil
}

// addNode add a node into a cluster which is aleardy start
func addNode(cluster []*Raft, tlsEnable bool) ([]*Raft, error) {
	peers := make(map[uint64]string)
	for i := 1; i <= 4; i++ {
		peers[uint64(i)] = fmt.Sprintf("localhost:%d", beginPort+2+3*(i-1))
	}
	cfg, err := newRaftConfg(4, beginPort+3*3, fmt.Sprintf("%s/4", getClusterPath()), peers, tlsEnable)
	if err != nil {
		return cluster, err
	}
	cfg.join = true
	node, err := NewRaft(cfg)
	if err != nil {
		return cluster, err
	}

	if err := node.Start(); err != nil {
		return cluster, err
	}

	cluster = append(cluster, node)

	return cluster, nil
}

func removeNode(cluster []*Raft, id uint64) ([]*Raft, error) {
	var node *Raft
	for i := range cluster {
		if cluster[i].cfg.id == id {
			node = cluster[i]
			break
		}
	}
	if node == nil {
		return cluster, errors.New("Failed to find node in cluster")
	}
	for i := 0; i < 10; i++ {
		leader := cluster[0].eraft.getLeader()
		if leader == 0 {
			time.Sleep(1 * time.Second)
		} else {
			var conn *grpc.ClientConn
			var err error
			if node.cfg.tls.enable {
				creds := credentials.NewTLS(&tls.Config{
					Certificates: []tls.Certificate{node.cfg.tls.cert},
					RootCAs:      node.cfg.tls.pool,
				})
				conn, err = grpc.Dial(fmt.Sprintf("localhost:%d", 12346+3*(leader-1)), grpc.WithTransportCredentials(creds), grpc.WithTimeout(2000*time.Millisecond))
			} else {
				conn, err = grpc.Dial(fmt.Sprintf("localhost:%d", 12346+3*(leader-1)), grpc.WithInsecure(), grpc.WithTimeout(2000*time.Millisecond))
			}
			if err != nil {
				return cluster, err
			}
			client := raftpb.NewRaftClient(conn)
			_, err = client.RemoveNode(context.Background(), &raftpb.NodeInfo{
				ID:        id,
				URL:       "localhost",
				ChainPort: int32(beginPort + 3*3),
			})
			if err != nil {
				return cluster, err
			}
			break
		}
	}
	// new cluster
	var nc = make([]*Raft, 0)
	for i := range cluster {
		if cluster[i].cfg.id != id {
			nc = append(nc, cluster[i])
		}
	}
	return nc, nil
}

func startCluster(cluster []*Raft) (err error) {
	var wg sync.WaitGroup
	var lock sync.Mutex

	for i := range cluster {
		wg.Add(1)
		node := cluster[i]
		go func() {
			defer wg.Done()
			if e := node.Start(); e != nil {
				lock.Lock()
				err = e
				lock.Unlock()
				return
			}
		}()
	}
	wg.Wait()

	return
}

func stopCluster(cluster []*Raft) error {
	for i := range cluster {
		cluster[i].Stop()
	}
	// for i := 0; i < 3*len(cluster); i++ {
	// 	if butil.HasConnection(beginPort + i) {
	// 		return fmt.Errorf("Port %d still has connection", beginPort+i)
	// 	}
	// }
	return nil
}

func restartNode(cluster []*Raft) error {
	for range cluster {
		node := cluster[util.RandNum(len(cluster))]
		fmt.Printf("[-]Kill node %d\n", node.cfg.id)
		node.Stop()
		time.Sleep(1000 * time.Millisecond)
		if err := node.Start(); err != nil {
			return err
		}
		fmt.Printf("[+]Start node %d\n", node.cfg.id)
	}
	return nil
}

// newRaftConfg will generate min need config for raft test, peers must include itself
func newRaftConfg(id uint64, chainPort int, dir string, peers map[uint64]string, tlsEnable bool) (*Config, error) {
	var tlsCfg config.TLSConfig
	if tlsEnable {
		// load pool
		pool := x509.NewCertPool()
		ca, err := ioutil.ReadFile(".certs/ca.pem")
		if err != nil {
			return nil, err
		}
		ok := pool.AppendCertsFromPEM(ca)
		if !ok {
			return nil, fmt.Errorf("Failed to load ca file")
		}
		// load cert
		cert, err := tls.LoadX509KeyPair(fmt.Sprintf(".certs/%d/localhost.pem", id), fmt.Sprintf(".certs/%d/localhost-key.pem", id))
		if err != nil {
			return nil, err
		}
		tlsCfg = config.TLSConfig{
			Enable:  true,
			Pool:    pool,
			Cert:    &cert,
			RawCert: fmt.Sprintf(".certs/%d/localhost.pem", id),
			CA:      ".certs/ca.pem",
			Key:     fmt.Sprintf(".certs/%d/localhost-key.pem", id),
		}
	} else {
		tlsCfg = config.TLSConfig{
			Enable: false,
		}
	}
	cfg := &config.Config{
		Address: "localhost",
		Port:    chainPort,
		TLS:     &tlsCfg,
		BlockChain: &config.BlockChainConfig{
			Path: dir,
			Raft: config.RaftConfig{
				Enable: true,
				ID:     id,
				Nodes:  peers,
			},
		},
	}

	return NewConfig(cfg)
}

func initSoloPath() {
	os.RemoveAll(getSoloPath())
}

func getSoloPath() string {
	return ".solo"
}

func initClusterPath() {
	os.RemoveAll(getClusterPath())
}

func getClusterPath() string {
	return ".cluster"
}
